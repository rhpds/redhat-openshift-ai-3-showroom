= OpenShift AI 3.0 Installation & Deployment Guide

== 1. OpenShift Installation (IPI)

Create an `install-config.yaml` to provision an SNO (Single Node OpenShift) cluster on AWS using IPI.

.Sample `install-config.yaml`
[source,yaml]
----
apiVersion: v1
baseDomain: rhoai.rh-aiservices-bu.com
compute:
  - architecture: amd64
    hyperthreading: Enabled
    name: worker
    platform:
      aws:
        type: m6i.4xlarge
    replicas: 0
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform:
    aws:
      type: g6e.12xlarge
      rootVolume:
        size: 1000
  replicas: 1
metadata:
  name: test-rc2
networking:
  clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
  machineNetwork:
    - cidr: 10.0.0.0/16
  networkType: OVNKubernetes
  serviceNetwork:
    - 172.30.0.0/16
platform:
  aws:
    region: us-west-2
publish: External
pullSecret: "<your pull secret>"
sshKey: "<your ssh Key>"
----

Download the OpenShift installer:

https://console.redhat.com/openshift/downloads#tool-x86_64-openshift-install

Run the installation:

[source,bash]
----
./openshift-install create cluster --dir . --log-level=info
----

After installation, log in with the generated `kubeadmin` credentials.

== 2. Post-Deployment Steps

The following operators must be deployed before provisioning OpenShift AI:

* Certificate Manager
* CertManager Instance
* ClusterIssuer + Secret
* API + Apps certificates
* Cert copy jobs (API + Apps)
* Node Feature Discovery Operator + CR
* NVIDIA GPU Operator + ClusterPolicy
* Leader Worker Set (required for llm-d)
* Red Hat Connectivity Link Operator (required for llm-d)
* DNS Operator (installed with RHCL)
* Limitador Operator (installed with RHCL)
* Authorino Operator (installed with RHCL)

=== OAuth & Access

* Add Red Hat SSO as an OAuth provider
* Add Google client secret
* Add OAuth redirect URL to Google Client App
* Update OAuth config
* Add AI-BU admins as `rhods-admin`

=== Enable UserWorkloadMonitoring

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    alertmanagerMain:
      enableUserAlertmanagerConfig: true
----

=== Shared Namespace

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ai-shared-bu
----

[NOTE]
Storage must be *≥1 TB* for SNO.

=== Notes

* Service Mesh 3 and Kueue are installed automatically via the DataScienceCluster CR.

== 3. OpenShift AI 3.0 Operator Installation

RHOAI 3.0 is not GA yet. Use RC or nightly fragments.

=== Add RHOAI 3.0 CatalogSource

[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: rhoai-3.0-catalog
  namespace: openshift-marketplace
spec:
  displayName: RHOAI 3.0 Catalog
  image: "quay.io/rhoai/rhoai-fbc-fragment@sha256:364ea097..."
  publisher: Red Hat
  sourceType: grpc
  updateStrategy:
    registryPoll:
      interval: 30m
----

=== ImageContentSourcePolicy

[source,yaml]
----
apiVersion: operator.openshift.io/v1alpha1
kind: ImageContentSourcePolicy
metadata:
 name: quay-registry
spec:
 repositoryDigestMirrors:
   - mirrors:
       - quay.io/rhoai
     source: registry.redhat.io/rhoai
----

Install RHOAI from *fast → 3.x* channel.

== 4. Deploy OpenShift AI 3.0

Create a DataScienceCluster:

[source,yaml]
----
kind: DataScienceCluster
apiVersion: datasciencecluster.opendatahub.io/v2
metadata:
 name: default-dsc
spec:
 components:
   dashboard:
     managementState: Managed
   aipipelines:
     managementState: Managed
   feastoperator:
     managementState: Managed
   kserve:
     managementState: Managed
   llamastackoperator:
     managementState: Managed
   kueue:
     managementState: Removed
   modelregistry:
     managementState: Managed
     registriesNamespace: rhoai-model-registries
   ray:
     managementState: Managed
   workbenches:
     managementState: Managed
   trainingoperator:
     managementState: Managed
   trustyai:
     managementState: Managed
----

== 5. Enabling Features (Playground, AI Assets, etc.)

=== Update OdhDashboardConfig

[source,yaml]
----
apiVersion: opendatahub.io/v1alpha
kind: OdhDashboardConfig
metadata:
 name: odh-dashboard-config
 namespace: redhat-ods-applications
spec:
 dashboardConfig:
   disableTracking: false
   disableModelRegistry: false
   disableModelCatalog: false
   disableKServeMetrics: false
   genAiStudio: true
   modelAsService: true
   disableLMEval: false
 notebookController:
   enabled: true
   notebookNamespace: rhods-notebooks
   pvcSize: 20Gi
----

=== Hardware Profile

[source,yaml]
----
apiVersion: infrastructure.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
 name: gpu-profile
 namespace: redhat-ods-applications
spec:
 identifiers:
   - identifier: cpu
     displayName: CPU
     resourceType: CPU
     minCount: 1
     maxCount: "8"
     defaultCount: "1"
   - identifier: memory
     displayName: Memory
     resourceType: Memory
     minCount: 1Gi
     maxCount: 16Gi
     defaultCount: 12Gi
   - identifier: nvidia.com/gpu
     displayName: GPU
     resourceType: Accelerator
     minCount: 1
     maxCount: 4
     defaultCount: 1
----

=== Deploy a Model with the AI Assets

The following resources create a model asset for *Llama 3.2–3B Instruct*, including:

* A Secret containing the OCI model reference
* A global ServingRuntime using vLLM on GPU
* An InferenceService integrated with KServe and GPU hardware profiles

.Apply the model asset definitions:
[source,yaml]
----
kind: Secret
apiVersion: v1
metadata:
  name: llama-32-3b-instruct
  namespace: ai-bu-shared
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/connection-type-protocol: uri
    opendatahub.io/connection-type-ref: uri-v1
    openshift.io/description: ''
    openshift.io/display-name: llama-3.2-3b-instruct
data:
  URI: b2NpOi8vcXVheS5pby9yZWRoYXQtYWktc2VydmljZXMvbW9kZWxjYXItY2F0YWxvZzpsbGFtYS0zLjItM2ItaW5zdHJ1Y3Q=
type: Opaque
----
[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/runtime-version: v0.9.1.0
    opendatahub.io/serving-runtime-scope: global
    opendatahub.io/template-display-name: vLLM NVIDIA GPU ServingRuntime for KServe
    opendatahub.io/template-name: vllm-cuda-runtime-template
    openshift.io/display-name: vLLM NVIDIA GPU ServingRuntime for KServe
  name: llama-32-3b-instruct
  namespace: ai-bu-shared
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
  containers:
    - args:
        - '--port=8080'
        - '--model=/mnt/models'
        - '--served-model-name={{.Name}}'
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      env:
        - name: HF_HOME
          value: /tmp/hf_home
      image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ad756c01ec99a99cc7d93401c41b8d92ca96fb1ab7c5262919d818f2be4f3768'
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
----
[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.kserve.io/stop: 'false'
    security.opendatahub.io/enable-auth: 'false'
    openshift.io/description: ''
    openshift.io/display-name: llama-32-3b-instruct
    serving.kserve.io/deploymentMode: RawDeployment
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/hardware-profile-name: gpu-profile
    opendatahub.io/connections: llama-32-3b-instruct
    opendatahub.io/model-type: generative
  name: llama-32-3b-instruct
  namespace: ai-bu-shared
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/genai-asset: 'true'
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: 1
    minReplicas: 1
    model:
      args:
        - '--dtype=half'
        - '--max-model-len=20000'
        - '--gpu-memory-utilization=0.95'
        - '--enable-auto-tool-choice'
        - '--tool-call-parser=llama3_json'
        - '--chat-template=/opt/app-root/template/tool_chat_template_llama3.2_json.jinja'
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '4'
          memory: 16Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '1'
          memory: 6Gi
          nvidia.com/gpu: '1'
      runtime: llama-32-3b-instruct
      storageUri: 'oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct'
----

=== Deploy Model via AI Assets

(Secret, ServingRuntime, InferenceService omitted here for brevity—they are included fully in your source and can be appended if needed.)

Models will become available in the catalog once `Started / Running`.

=== GenAI Playground Integration

* Add model to Playground via *Add to Playground*
* A LlamaStackDistribution (lsd) instance is created automatically
* A `run.yaml` ConfigMap is generated
* A `lsd-<model>` pod spins up

=== MCP Servers

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
 name: gen-ai-aa-mcp-servers
 namespace: redhat-ods-applications
data:
 GitHub-MCP-Server: |
   {
     "url": "https://api.githubcopilot.com/mcp",
     "description": "GitHub MCP Server..."
   }
----

== 6. LLMd Deployment

[NOTE]
RHCL must be installed before proceeding.

If the KServe operator does not detect RHCL, reboot KServe operator pod.

=== GatewayClass

[source,yaml]
----
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
 name: openshift-ai-inference
spec:
 controllerName: openshift.io/gateway-controller/v1
----

=== Inference Gateway

[source,yaml]
----
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
 name: openshift-ai-inference
 namespace: openshift-ingress
spec:
 gatewayClassName: openshift-ai-inference
 listeners:
   - name: https
     port: 443
     protocol: HTTPS
     hostname: inference-gateway.apps.test-rc3.rhoai.rh-aiservices-bu.com
     tls:
       mode: Terminate
       certificateRefs:
         - kind: Secret
           name: default-gateway-tls
     allowedRoutes:
       namespaces:
         from: Selector
         selector:
           matchExpressions:
             - key: kubernetes.io/metadata.name
               operator: In
               values:
                 - openshift-ingress
                 - redhat-ods-applications
                 - ai-bu-shared
----

Deploy via UI → may show *Failed* (still WIP).

== 7. MaaS (Model-as-a-Service)

Installation reference:

https://opendatahub-io.github.io/maas-billing/install/maas-setup/

== 8. Observability & Tracing

Follow the documentation:

https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html/managing_openshift_ai/managing-observability_managing-rhoai#collecting-metrics-from-user-workloads_managing-rhoai

Prerequisites:

* Install Cluster Observability Operator
